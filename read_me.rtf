{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww37900\viewh20740\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # TimeSeriesClassification\
This repository is created for experiments on time series classification. \
\
Modifications made on the traditional Transformer model used for NLP. \
Traditional transformer models get a sequence as input and I take the advantage of this sequence input.\
\
\
You can find hyper parameters in model_summary.py file. \
The output for model summary is represented below. The dimensions are batch\
\
before input layer: torch.Size([7, 136, 1])\
after input layer: torch.Size([7, 136, 100])\
after pos_emb: torch.Size([7, 136, 100])\
in Multi Head Attention Q,K,V: torch.Size([7, 136, 100])\
in splitted Multi Head Attention Q,K,V: torch.Size([7, 4, 136, 25])\
in Scale Dot Product, k_t size: torch.Size([7, 4, 25, 136])\
in Scale Dot Product, score size: torch.Size([7, 4, 136, 136])\
in Scale Dot Product, score size after softmax : torch.Size([7, 4, 136, 136])\
in Scale Dot Product, v size: torch.Size([7, 4, 136, 25])\
in Scale Dot Product, v size after matmul: torch.Size([7, 4, 136, 25])\
in Multi Head Attention, score value size: torch.Size([7, 4, 136, 25])\
in Multi Head Attention, score value size after concat : torch.Size([7, 136, 100])\
in encoder layer : torch.Size([7, 136, 100])\
in encoder after norm layer : torch.Size([7, 136, 100])\
in encoder after ffn : torch.Size([7, 136, 100])\
in Multi Head Attention Q,K,V: torch.Size([7, 136, 100])\
in splitted Multi Head Attention Q,K,V: torch.Size([7, 4, 136, 25])\
in Scale Dot Product, k_t size: torch.Size([7, 4, 25, 136])\
in Scale Dot Product, score size: torch.Size([7, 4, 136, 136])\
in Scale Dot Product, score size after softmax : torch.Size([7, 4, 136, 136])\
in Scale Dot Product, v size: torch.Size([7, 4, 136, 25])\
in Scale Dot Product, v size after matmul: torch.Size([7, 4, 136, 25])\
in Multi Head Attention, score value size: torch.Size([7, 4, 136, 25])\
in Multi Head Attention, score value size after concat : torch.Size([7, 136, 100])\
in encoder layer : torch.Size([7, 136, 100])\
in encoder after norm layer : torch.Size([7, 136, 100])\
in encoder after ffn : torch.Size([7, 136, 100])\
in classification head : torch.Size([7, 136, 100])\
in classification head after seq: torch.Size([7, 5])\
after cls_res: torch.Size([7, 5]) \
\
==============================================================================================================\
Layer (type:depth-idx)                                       Output Shape              Param #\
==============================================================================================================\
Transformer                                                  [7, 5]                    --\
\uc0\u9500 \u9472 Linear: 1-1                                                [7, 136, 100]             200\
\uc0\u9500 \u9472 PostionalEncoding: 1-2                                     [7, 136, 100]             --\
\uc0\u9474     \u9492 \u9472 Dropout: 2-1                                          [7, 136, 100]             --\
\uc0\u9500 \u9472 Encoder: 1-3                                               [7, 136, 100]             --\
\uc0\u9474     \u9492 \u9472 ModuleList: 2-2                                       --                        --\
\uc0\u9474     \u9474     \u9492 \u9472 EncoderLayer: 3-1                                [7, 136, 100]             143,812\
\uc0\u9474     \u9474     \u9492 \u9472 EncoderLayer: 3-2                                [7, 136, 100]             143,812\
\uc0\u9500 \u9472 ClassificationHead: 1-4                                    [7, 5]                    --\
\uc0\u9474     \u9492 \u9472 LayerNorm: 2-3                                        [7, 136, 100]             200\
\uc0\u9474     \u9492 \u9472 Sequential: 2-4                                       [7, 5]                    --\
\uc0\u9474     \u9474     \u9492 \u9472 Flatten: 3-3                                     [7, 13600]                --\
\uc0\u9474     \u9474     \u9492 \u9472 Linear: 3-4                                      [7, 512]                  6,963,712\
\uc0\u9474     \u9474     \u9492 \u9472 ReLU: 3-5                                        [7, 512]                  --\
\uc0\u9474     \u9474     \u9492 \u9472 Linear: 3-6                                      [7, 256]                  131,328\
\uc0\u9474     \u9474     \u9492 \u9472 Linear: 3-7                                      [7, 5]                    1,285\
==============================================================================================================\
Total params: 7,384,349\
Trainable params: 7,384,349\
Non-trainable params: 0\
Total mult-adds (M): 51.68\
==============================================================================================================\
Input size (MB): 0.00\
Forward/backward pass size (MB): 20.03\
Params size (MB): 29.54\
Estimated Total Size (MB): 49.57\
==============================================================================================================}