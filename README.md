# TimeSeries Classification and Forecasting
This repository was created for experiments on time series classification and forecasting.  
The underlying reason why transformer has been chosen for classification and forecastion of timeseries is transformers are faster than RNN-based models as all the input is ingested once. Training LSTMs is harder when compared with transformer networks, since the number of parameters is a lot more in LSTM networks.  
Currently, the model has been modified for the classification task. In the coming days, it will be further modified for forecasting.  
Transformer models used in NLP generally have an encoder and decoder for transforming input data (Query) into the expected output (Value). The main concept of transformer models is based on multi-head attention units, which generate a value representing the relation between Query and Key values within an energy matrix. The Query and Key values are generally identical to the input vector, and the multi-head attention mechanism generates a Value matrix that includes energy values representing the relation value between each point for the input vector (NxN).  

In this project, the Value matrice generated by multi head attention has been used for classification. A classification mechanism has been added to the end of the encoder part of the model. 

Modifications have been made to the traditional Transformer model used in NLP.

Traditional transformer models get a sequence as input and also have a decoder part for transforming the input value to the expected output.  
In this model, the encoder part of transformer model has been used for classification task. The encoder part of transformer model includes self attention mechanism. The self attention mechanism helps the encoder to understand relation between each data points.


You can find hyper parameters in model_summary.py file. 
The output for model summary is represented below. 
![image](https://user-images.githubusercontent.com/6734818/225657838-b3b211b1-9412-4752-ab98-059051f61060.png)


